function [classifiers, neighbor_infor] = f_train_models_pc(X_train, y_train, sim,...
    nSteps, lazi, high, low, train, test, y, model_type, err_lim)
% train models for the PC (personalized classifier) classifier
% X: training data
% Y: class labels
% sim: similarity matrix of instances, can be get by "sim = IDFsim(X)" or ,
%  sim = squareform(pdist(X, 'cosine')); sim = -sim;
%
%  since 'sim' might be pre-computed, thus I just let the sim be passed
%  into this function.



% X_posi = X(Y==1, :);
% X_nega = X(Y==0, :);
% Y_posi = Y(Y==1, :);
% Y_nega = Y(Y==0, :);


% Get the RWR weighted net from sim
% nSteps: random walk steps
% nSteps = 1000;

% lazi: random walk restart probability.
% lazi = 0.3;

% sim_sub = sim( train & (y==1), train & (y==1));
sim_sub = sim( train, train);

% Generate the Random-Walk net.
% default nSteps = 1000;
rw_mat = f_sim_to_rw_mat(sim_sub, nSteps, lazi);

% Compute the number of neighbors and the corresponding cutoffs
% high / low: average neighbor size's bound:
% high=7;
% low =2;

szs = f_gen_neighbor_count_cutoff(size(sim_sub,1),...
    high,low);
cutoffs = getCutoffs(rw_mat, szs);
minNeighbors = min(szs);

% rw_net is the weighted net. neighbor_infor is generated by rw_net, and it
%  is a collection of nets with different cutoffs.
neighbor_infor = cell(1, length(cutoffs));
for i=1:length(cutoffs)
    rwTemp = rw_mat>cutoffs(i);
    neighb = f_fill_rand_neighbors(rwTemp, minNeighbors);
    %neighbor_infor_sub{1, ct} = 
    neighbor_infor{1, i} = sparse(neighb);
end

% prediction cutoffs
% if a positive sequence being predicted lower than 0.65, then it is wrong.
% ct_posi = 0.6;
% if a negative sequence being predicted higher than 0.4, then it is wrong.
% ct_nega = 0.4;
% I found that some times, there might be no good model if the cutoff is
%  too strict, so I need to make the cutoff less strict. 

% in a lot of cases, the predictions of p and n may all bigger than 0.5,
%  thus there would be no model to be selected at all. I have to use another
%  rule to select some models. 
%ct_pn_diff = 0.8;


% =========================== Build Models ================================
% #model groups = #instances
classifiers = cell(1, size(X_train,1));
for node = 1: size(X_train,1)
    % node_classifiers is all the classifiers of the instance. Size: n x 1
    node_classifiers = {};
    for i=1:length(cutoffs)
        % node,
        % len = size(X_posi, 1),
        % x_node_p = X_posi(node, :);
        % x_node_n = X_nega(node, :);
        
        % x_node: a training instance
        x_node = X_train(node, :);
        y_train_node = y_train(node, :);
        
        
        % nb_loc: neighbor locations
        nb_loc = find(neighbor_infor{1, i}(:, node));
        
        % X_train = [X_posi(nb_loc, :);  X_nega(nb_loc, :)];
        % y_train = [Y_posi(nb_loc, 1);  Y_nega(nb_loc, 1)];
        X_train_cur = X_train(nb_loc, :);
        y_train_cur = y_train(nb_loc, 1);
        % X_test = [x_node_p; x_node_n];
        X_test_cur = x_node;
        y_test_cur = y_train_node;

        % [y_predicted,~]=f_multi_lin_regress_predict(X_train, ...
        %     y_train, X_test);
        m = f_train_model(X_train_cur, y_train_cur, model_type);
        y_predicted = f_predict_simple(X_test_cur, m, model_type);
        
        % y_predicted
        % y_predicted(1) is the predicted result of the positive
        %  instance, while y_predicted(2) is the predicted result for
        %  the negative instance. 
        % %  if (y_predicted(1) > ct_posi && y_predicted(2) < ct_nega)

        % prediction err cannot be larger than 0.3
        % err_lim = 0.3;
        
        % err = abs(y_predicted(1) - y_train_cur );
        err = 0;
        if y_test_cur == 1
            err = y_test_cur - y_predicted(1);
        elseif y_test_cur == 0
            % err = y_predicted(1) -y_train_cur;
            err = y_predicted(1) ;
        end
        
        if err < err_lim
            % if passed the cutoffs, then the model is good.
            % retrain the model with the node.
            X_train2 = [x_node; X_train_cur];
            y_train2 = [y_train(node); y_train_cur];
            % b = regress(y_train2, [X_train2, ones(size(y_train2))]);
            m = f_train_model(X_train2, y_train2, model_type);
            
            node_classifiers=[node_classifiers; {m}];
        end
    end
    classifiers{1, node} = node_classifiers;
    node,
end



end

